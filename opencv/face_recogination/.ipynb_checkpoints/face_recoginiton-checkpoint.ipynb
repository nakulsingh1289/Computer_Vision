{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 : Extract embedding from our dataset\n",
    "\n",
    "# import required packages\n",
    "\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2 as cv\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector....\n"
     ]
    }
   ],
   "source": [
    "# laod our face detector from disk\n",
    "\n",
    "print('[INFO] loading face detector....')\n",
    "\n",
    "detector = cv.dnn.readNetFromCaffe('deploy.prototxt', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "# deply.prototxt defines the architecture of model\n",
    "# .caffemodel have the actual weights\n",
    "\n",
    "# load our face embedding model from disk\n",
    "\n",
    "embedder = cv.dnn.readNetFromTorch('openface_nn4.small2.v1.t7')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] quantifying faces...')\n",
    "\n",
    "# initialize our lists of facial embeddings and \n",
    "# correspondding peoples names\n",
    "\n",
    "knownEmd = []\n",
    "knownNames = []\n",
    "\n",
    "# initialize the total number of faces processed\n",
    "total = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of paths of training images\n",
    "\n",
    "imagepath=[]\n",
    "imagePaths= list(os.listdir(\"dataset/nakul\"))\n",
    "\n",
    "for i in imagePaths:\n",
    "    image= 'dataset/nakul/'+i\n",
    "    imagepath.append(image)\n",
    "imagePaths2= list(os.listdir(\"dataset/unknown\"))\n",
    "for i in imagePaths2:\n",
    "    image= 'dataset/unknown/'+i\n",
    "    imagepath.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing image 1/22\n",
      " [INFO] processing image 2/22\n",
      " [INFO] processing image 3/22\n",
      " [INFO] processing image 4/22\n",
      " [INFO] processing image 5/22\n",
      " [INFO] processing image 6/22\n",
      " [INFO] processing image 7/22\n",
      " [INFO] processing image 8/22\n",
      " [INFO] processing image 9/22\n",
      " [INFO] processing image 10/22\n",
      " [INFO] processing image 11/22\n",
      " [INFO] processing image 12/22\n",
      " [INFO] processing image 13/22\n",
      " [INFO] processing image 14/22\n",
      " [INFO] processing image 15/22\n",
      " [INFO] processing image 16/22\n",
      " [INFO] processing image 17/22\n",
      " [INFO] processing image 18/22\n",
      " [INFO] processing image 19/22\n",
      " [INFO] processing image 20/22\n",
      " [INFO] processing image 21/22\n",
      " [INFO] processing image 22/22\n",
      "Total number of processed images=  22\n"
     ]
    }
   ],
   "source": [
    "#loop over the image paths\n",
    "\n",
    "for (i , imgpath) in enumerate(imagepath):\n",
    "\n",
    "    # extract the person name from the image path\n",
    "    print(' [INFO] processing image {}/{}'.format(i+1, len(imagepath)))\n",
    "    name = imgpath.split('/')[-2]\n",
    "\n",
    "    image= cv.imread(imgpath)\n",
    "    image= imutils.resize(image, width=600)\n",
    "    (h,w)= image.shape[:2]\n",
    "\n",
    "    imageblob= cv.dnn.blobFromImage( cv.resize(image,(300,300)), 1.0, (300, 300), (104, 177, 123), swapRB=False, crop= False)\n",
    "\n",
    "    #apply Opencv deep learing based face detector to localize faces in the input image\n",
    "\n",
    "    detector.setInput(imageblob)\n",
    "\n",
    "    dectections= detector.forward()\n",
    "\n",
    "    #ensure atleast one face was found\n",
    "\n",
    "    if len( dectections >0):\n",
    "\n",
    "        i= np.argmax(dectections[0,0,:,2])\n",
    "        confidence= dectections[0,0,i,2]\n",
    "\n",
    "        if confidence > 0.5:\n",
    "\n",
    "            box= dectections[0,0,i,3:7]* np.array([w,h,w,h])\n",
    "            (startX, startY, endX, endY) = box.astype('int')\n",
    "              \n",
    "            # cropping face from the image for extracting embedding\n",
    "\n",
    "            face= image[startY:endY, startX: endX]\n",
    "            (fh, fw)= face.shape[:2]\n",
    "            #cv.imshow(\"image\", face)\n",
    "            #cv.waitKey(0)\n",
    "            if (fw< 20 or fh <20):\n",
    "                continue      # skip if detected face is very small\n",
    "            \n",
    "            faceblob= cv.dnn.blobFromImage(face, 1.0/255, (96,96), (0,0,0), swapRB=True, crop= False)\n",
    "\n",
    "            embedder.setInput(faceblob)\n",
    "            vec= embedder.forward()\n",
    "\n",
    "            #print(vec)\n",
    "\n",
    "            knownNames.append(name)\n",
    "            knownEmd.append(vec.flatten())\n",
    "            total= total+1\n",
    "\n",
    "print(\"Total number of processed images= \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face embedding....\n",
      "{INFO] encoding labels...\n",
      "[INFO] training model....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2: TRAIN OUR MODEL\n",
    "\n",
    "# loads the face embedding\n",
    "\n",
    "print('[INFO] loading face embedding....')\n",
    "data= { 'embeddings':knownEmd, 'names':knownNames}\n",
    "# encodes the labels\n",
    "\n",
    "print('{INFO] encoding labels...')\n",
    "le= LabelEncoder()\n",
    "labels= le.fit_transform(data['names'])\n",
    "#print(labels)\n",
    "\n",
    "# train the model used to accept the 120-d emmbedding of the face and\n",
    "# then produce the actual face recognition \n",
    "\n",
    "print('[INFO] training model....')\n",
    "recognizer = SVC(C=1.0, kernel= \"linear\", probability= True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3: recognize faces with openCV\n",
    "# load image to be recognized \n",
    "image = input(\"Enter name or path of image to be recognised: \")\n",
    "\n",
    "image = cv.imread(image)\n",
    "image = imutils.resize( image, width= 600)\n",
    "#cv.imshow(\"image\", image)\n",
    "#cv.waitKey(0)\n",
    "\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "imageblob = cv.dnn.blobFromImage(cv.resize(image, (300,300)), 1.0, (300,300), (104, 177, 123), swapRB=False, crop= False )\n",
    "\n",
    "detector.setInput(imageblob)\n",
    "dectections= detector.forward()\n",
    "\n",
    "for i in range(0, dectections.shape[2]):\n",
    "    confidence  = dectections[0,0,i,2]\n",
    "\n",
    "    if confidence > 0.5:\n",
    "        box = dectections[0,0,i,3:7]* np.array([w,h,w,h])\n",
    "        (startX, startY, endX, endY) = box.astype('int')\n",
    "        face = image[ startY:endY, startX:endX]\n",
    "        (fh, fw) = face.shape[:2]\n",
    "        if fw<20 or fh<20:\n",
    "            continue\n",
    "        \n",
    "        faceblob= cv.dnn.blobFromImage(face, 1.0/255, (96,96), (0,0,0), swapRB=True, crop= False)\n",
    "        embedder.setInput(faceblob)\n",
    "        vec = embedder.forward()\n",
    "\n",
    "        preds= recognizer.predict_log_proba(vec)[0]\n",
    "        j= np.argmax(preds)\n",
    "        proba= preds[j]\n",
    "        name= le.classes_[j]\n",
    "\n",
    "        text= \"{} : {:.2f}%\".format(name, proba *100)\n",
    "        y= startY-10 if startY-10 >10 else startY+10\n",
    "\n",
    "        cv.rectangle(image, (startX, startY), (endX, endY), (0,0,255), 2)\n",
    "        cv.putText(image, text, (startX, y), cv.FONT_HERSHEY_SIMPLEX, 0.45, (0,0,255), 2)\n",
    "\n",
    "cv.imshow(\"Image\", image)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit7fa94754980449c5b815f20e746f1f4d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
